# PostgreSQL Docker ç’°å¢ƒ

é–‹ç™ºãƒ»å­¦ç¿’ç”¨ã® PostgreSQL Docker ç’°å¢ƒã§ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®è¨­å®šã¨ã€ç®¡ç†ãƒ„ãƒ¼ãƒ«ï¼ˆpgAdminï¼‰ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ“‹ ç›®æ¬¡

- [ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ](#ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ)
- [æ§‹æˆ](#æ§‹æˆ)
- [æ¥ç¶šæƒ…å ±](#æ¥ç¶šæƒ…å ±)
- [åŸºæœ¬æ“ä½œ](#åŸºæœ¬æ“ä½œ)
- [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](#ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°)
- [ç›£è¦–ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹](#ç›£è¦–ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹)
- [ç™ºå±•çš„ãªä½¿ã„æ–¹](#ç™ºå±•çš„ãªä½¿ã„æ–¹)
- [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

---

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. èµ·å‹•

```bash
cd tools/postgres
docker-compose up -d
```

### 2. æ¥ç¶šç¢ºèª

```bash
# PostgreSQLã«æ¥ç¶š
docker-compose exec postgres psql -U postgres -d testdb

# ã¾ãŸã¯
psql -h localhost -U postgres -d testdb
```

### 3. åœæ­¢

```bash
docker-compose down

# ãƒ‡ãƒ¼ã‚¿ã‚‚å‰Šé™¤ã™ã‚‹å ´åˆ
docker-compose down -v
```

---

## ğŸ“¦ æ§‹æˆ

```
tools/postgres/
â”œâ”€â”€ Dockerfile              # PostgreSQLã‚¤ãƒ¡ãƒ¼ã‚¸å®šç¾©
â”œâ”€â”€ docker-compose.yml      # ã‚µãƒ¼ãƒ“ã‚¹æ§‹æˆ
â”œâ”€â”€ postgresql.conf         # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ init/                   # åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ 01_create_extensions.sql
â”‚   â””â”€â”€ 02_create_sample_tables.sql
â””â”€â”€ README.md
```

### å«ã¾ã‚Œã‚‹ã‚µãƒ¼ãƒ“ã‚¹

- **PostgreSQL 16**: ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- **pgAdmin 4**: Web ãƒ™ãƒ¼ã‚¹ã®ç®¡ç†ãƒ„ãƒ¼ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

---

## ğŸ” æ¥ç¶šæƒ…å ±

### PostgreSQL

| é …ç›®         | å€¤        |
| ------------ | --------- |
| ãƒ›ã‚¹ãƒˆ       | localhost |
| ãƒãƒ¼ãƒˆ       | 5432      |
| ãƒ¦ãƒ¼ã‚¶ãƒ¼     | postgres  |
| ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰   | postgres  |
| ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ | testdb    |

**æ¥ç¶šæ–‡å­—åˆ—ä¾‹:**

```
postgresql://postgres:postgres@localhost:5432/testdb
```

### pgAdmin

- URL: http://localhost:5050
- Email: admin@example.com
- Password: admin

---

## ğŸ› ï¸ åŸºæœ¬æ“ä½œ

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†

```sql
-- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸€è¦§
\l

-- ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
\dt

-- ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ç¢ºèª
\d users

-- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
CREATE DATABASE mydb;

-- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‰Šé™¤
DROP DATABASE mydb;
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ

```sql
-- å®Ÿè¡Œä¸­ã®ã‚¯ã‚¨ãƒªç¢ºèª
SELECT pid, usename, application_name, state, query, query_start
FROM pg_stat_activity
WHERE state != 'idle'
ORDER BY query_start;

-- ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªçµ±è¨ˆï¼ˆpg_stat_statementsï¼‰
SELECT
    calls,
    total_exec_time,
    mean_exec_time,
    query
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;
```

---

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### 1. ãƒ¡ãƒ¢ãƒªè¨­å®šã®èª¿æ•´

ç¾åœ¨ã®è¨­å®šã¯ **2GB ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª** ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚

#### ãƒ¡ãƒ¢ãƒªåˆ¥æ¨å¥¨è¨­å®š

**4GB ãƒ¡ãƒ¢ãƒªã®å ´åˆ** (`postgresql.conf` ã‚’ç·¨é›†):

```conf
shared_buffers = 1GB
effective_cache_size = 3GB
work_mem = 32MB
maintenance_work_mem = 256MB
```

**8GB ãƒ¡ãƒ¢ãƒªã®å ´åˆ**:

```conf
shared_buffers = 2GB
effective_cache_size = 6GB
work_mem = 64MB
maintenance_work_mem = 512MB
```

**è¨ˆç®—å¼:**

- `shared_buffers`: ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªã® 25%
- `effective_cache_size`: ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªã® 50-75%
- `work_mem`: (RAM - shared_buffers) / (max_connections \* 3)
- `maintenance_work_mem`: ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªã® 5-10%ï¼ˆæœ€å¤§ 2GBï¼‰

### 2. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥ã®æœ€é©åŒ–

#### SSD ã®å ´åˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

```conf
random_page_cost = 1.1
effective_io_concurrency = 200
```

#### HDD ã®å ´åˆ

```conf
random_page_cost = 4.0
effective_io_concurrency = 2
```

### 3. æ¥ç¶šæ•°ã®èª¿æ•´

```conf
max_connections = 100        # è»½ã„è² è·
max_connections = 200        # ä¸­ç¨‹åº¦ã®è² è·
max_connections = 300        # é«˜è² è·
```

**æ³¨æ„**: æ¥ç¶šæ•°ã‚’å¢—ã‚„ã™å ´åˆã¯ `work_mem` ã‚’æ¸›ã‚‰ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

### 4. ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã®æœ€é©åŒ–

```sql
-- çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
ANALYZE;

-- ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ã®çµ±è¨ˆæ›´æ–°
VACUUM ANALYZE table_name;

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æœ€é©åŒ–
REINDEX INDEX index_name;

-- ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã®ç¢ºèª
EXPLAIN ANALYZE
SELECT * FROM users WHERE email = 'user1@example.com';
```

### 5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥

```sql
-- B-tree ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€ä¸€èˆ¬çš„ãªæ¤œç´¢ç”¨ï¼‰
CREATE INDEX idx_users_email ON users(email);

-- éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ¡ä»¶ä»˜ãï¼‰
CREATE INDEX idx_active_users ON users(email) WHERE active = true;

-- è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_users_name_email ON users(last_name, first_name, email);

-- GIN ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆé…åˆ—ãƒ»JSONBç”¨ï¼‰
CREATE INDEX idx_tags_gin ON articles USING GIN(tags);

-- GiST ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå…¨æ–‡æ¤œç´¢ç”¨ï¼‰
CREATE INDEX idx_content_gist ON articles USING GiST(to_tsvector('english', content));

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½¿ç”¨çŠ¶æ³ç¢ºèª
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;
```

---

## ğŸ“Š ç›£è¦–ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

### 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºã®ç›£è¦–

```sql
-- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º
SELECT
    pg_database.datname,
    pg_size_pretty(pg_database_size(pg_database.datname)) AS size
FROM pg_database
ORDER BY pg_database_size(pg_database.datname) DESC;

-- ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

### 2. æ¥ç¶šæ•°ã®ç›£è¦–

```sql
-- ç¾åœ¨ã®æ¥ç¶šæ•°
SELECT count(*) FROM pg_stat_activity;

-- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ¥æ¥ç¶šæ•°
SELECT
    datname,
    count(*) as connections
FROM pg_stat_activity
GROUP BY datname
ORDER BY connections DESC;
```

### 3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡

```sql
-- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ï¼ˆ95%ä»¥ä¸ŠãŒç†æƒ³ï¼‰
SELECT
    sum(heap_blks_read) as heap_read,
    sum(heap_blks_hit) as heap_hit,
    sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratio
FROM pg_statio_user_tables;
```

### 4. ãƒ‡ãƒƒãƒ‰ã‚¿ãƒ—ãƒ«ã®ç¢ºèª

```sql
-- ãƒ‡ãƒƒãƒ‰ã‚¿ãƒ—ãƒ«ã®ç¢ºèª
SELECT
    schemaname,
    tablename,
    n_live_tup,
    n_dead_tup,
    round(n_dead_tup * 100.0 / NULLIF(n_live_tup + n_dead_tup, 0), 2) AS dead_ratio
FROM pg_stat_user_tables
WHERE n_dead_tup > 0
ORDER BY n_dead_tup DESC;
```

### 5. VACUUM ã®å®Ÿè¡Œ

```sql
-- é€šå¸¸ã® VACUUM
VACUUM table_name;

-- å®Œå…¨ãª VACUUMï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãƒ­ãƒƒã‚¯ç™ºç”Ÿï¼‰
VACUUM FULL table_name;

-- ANALYZE ã‚‚åŒæ™‚å®Ÿè¡Œ
VACUUM ANALYZE table_name;

-- è‡ªå‹• VACUUM ã®è¨­å®šç¢ºèª
SELECT
    relname,
    last_vacuum,
    last_autovacuum,
    last_analyze,
    last_autoanalyze
FROM pg_stat_user_tables
ORDER BY last_autovacuum;
```

---

## ğŸš€ ç™ºå±•çš„ãªä½¿ã„æ–¹

### 1. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°

å¤§è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç®¡ç†ã«æœ‰åŠ¹ã§ã™ã€‚

```sql
-- ç¯„å›²ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆæ—¥ä»˜åˆ¥ï¼‰
CREATE TABLE measurements (
    id SERIAL,
    measured_at TIMESTAMP NOT NULL,
    temperature FLOAT,
    humidity FLOAT
) PARTITION BY RANGE (measured_at);

-- æœˆåˆ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆ
CREATE TABLE measurements_2024_01 PARTITION OF measurements
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE measurements_2024_02 PARTITION OF measurements
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- ãƒªã‚¹ãƒˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆåœ°åŸŸåˆ¥ï¼‰
CREATE TABLE sales (
    id SERIAL,
    region TEXT,
    amount DECIMAL
) PARTITION BY LIST (region);

CREATE TABLE sales_asia PARTITION OF sales
FOR VALUES IN ('Japan', 'China', 'Korea');

CREATE TABLE sales_europe PARTITION OF sales
FOR VALUES IN ('UK', 'Germany', 'France');
```

### 2. ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

#### ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

`docker-compose.yml` ã«è¿½åŠ :

```yaml
postgres-replica:
  build: .
  container_name: personal_library_postgres_replica
  environment:
    POSTGRES_USER: postgres
    POSTGRES_PASSWORD: postgres
    PGDATA: /var/lib/postgresql/data/pgdata
  ports:
    - "5433:5432"
  command: |
    bash -c "
      until pg_basebackup --pgdata=/var/lib/postgresql/data/pgdata -R --slot=replication_slot --host=postgres --port=5432
      do
        echo 'Waiting for primary to connect...'
        sleep 1s
      done
      echo 'Backup done, starting replica...'
      postgres
    "
  depends_on:
    - postgres
  networks:
    - postgres_network
```

ãƒ—ãƒ©ã‚¤ãƒãƒªã‚µãƒ¼ãƒãƒ¼ã®è¨­å®šï¼ˆ`postgresql.conf`ï¼‰:

```conf
wal_level = replica
max_wal_senders = 10
max_replication_slots = 10
hot_standby = on
```

### 3. ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆPgBouncerï¼‰

`docker-compose.yml` ã«è¿½åŠ :

```yaml
pgbouncer:
  image: pgbouncer/pgbouncer:latest
  container_name: personal_library_pgbouncer
  environment:
    DATABASES_HOST: postgres
    DATABASES_PORT: 5432
    DATABASES_USER: postgres
    DATABASES_PASSWORD: postgres
    DATABASES_DBNAME: testdb
    PGBOUNCER_POOL_MODE: transaction
    PGBOUNCER_MAX_CLIENT_CONN: 1000
    PGBOUNCER_DEFAULT_POOL_SIZE: 25
  ports:
    - "6432:6432"
  depends_on:
    - postgres
  networks:
    - postgres_network
```

æ¥ç¶š: `postgresql://postgres:postgres@localhost:6432/testdb`

### 4. å…¨æ–‡æ¤œç´¢

```sql
-- å…¨æ–‡æ¤œç´¢ç”¨ã‚«ãƒ©ãƒ è¿½åŠ 
ALTER TABLE articles ADD COLUMN search_vector tsvector;

-- ãƒˆãƒªã‚¬ãƒ¼ã§è‡ªå‹•æ›´æ–°
CREATE TRIGGER tsvector_update BEFORE INSERT OR UPDATE
ON articles FOR EACH ROW EXECUTE FUNCTION
tsvector_update_trigger(search_vector, 'pg_catalog.english', title, content);

-- GINã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
CREATE INDEX idx_search_vector ON articles USING GIN(search_vector);

-- æ¤œç´¢å®Ÿè¡Œ
SELECT title, content
FROM articles
WHERE search_vector @@ to_tsquery('english', 'postgresql & performance');

-- æ—¥æœ¬èªå…¨æ–‡æ¤œç´¢ï¼ˆpg_bigm æ‹¡å¼µä½¿ç”¨ï¼‰
CREATE EXTENSION pg_bigm;
CREATE INDEX idx_content_bigm ON articles USING gin (content gin_bigm_ops);
SELECT * FROM articles WHERE content LIKE '%PostgreSQL%';
```

### 5. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆAI/MLç”¨ï¼‰

pgvector æ‹¡å¼µã‚’ä½¿ç”¨:

```sql
-- æ‹¡å¼µæ©Ÿèƒ½ã®æœ‰åŠ¹åŒ–
CREATE EXTENSION vector;

-- ãƒ™ã‚¯ãƒˆãƒ«ã‚«ãƒ©ãƒ ã‚’æŒã¤ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE embeddings (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding vector(1536)  -- OpenAIåŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒæ•°
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆIVFFlatï¼‰
CREATE INDEX ON embeddings USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- é¡ä¼¼æ¤œç´¢
SELECT content
FROM embeddings
ORDER BY embedding <-> '[0.1, 0.2, ...]'::vector
LIMIT 5;
```

### 6. JSONB ã®æ´»ç”¨

```sql
-- JSONBã‚«ãƒ©ãƒ ã‚’æŒã¤ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name TEXT,
    attributes JSONB
);

-- GINã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_attributes ON products USING GIN(attributes);

-- ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
INSERT INTO products (name, attributes) VALUES
('Laptop', '{"brand": "Dell", "ram": 16, "storage": "512GB SSD"}'),
('Mouse', '{"brand": "Logitech", "wireless": true, "dpi": 1600}');

-- ã‚¯ã‚¨ãƒª
SELECT * FROM products WHERE attributes @> '{"brand": "Dell"}';
SELECT * FROM products WHERE attributes->>'wireless' = 'true';
SELECT * FROM products WHERE attributes->'ram' > '8';
```

### 7. ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼

```sql
-- ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼ã®ä½œæˆ
CREATE MATERIALIZED VIEW user_statistics AS
SELECT
    date_trunc('day', created_at) as day,
    count(*) as user_count,
    count(*) FILTER (WHERE email LIKE '%@gmail.com') as gmail_users
FROM users
GROUP BY day;

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
CREATE INDEX idx_user_stats_day ON user_statistics(day);

-- ãƒ‡ãƒ¼ã‚¿æ›´æ–°
REFRESH MATERIALIZED VIEW user_statistics;

-- åŒæ™‚å®Ÿè¡Œå¯èƒ½ãªæ›´æ–°
REFRESH MATERIALIZED VIEW CONCURRENTLY user_statistics;
```

### 8. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æœ€é©åŒ–

TimescaleDB ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ:

```sql
-- æ‹¡å¼µæ©Ÿèƒ½ã®æœ‰åŠ¹åŒ–
CREATE EXTENSION timescaledb;

-- ãƒã‚¤ãƒ‘ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ
CREATE TABLE sensor_data (
    time TIMESTAMPTZ NOT NULL,
    sensor_id INTEGER,
    temperature DOUBLE PRECISION,
    humidity DOUBLE PRECISION
);

SELECT create_hypertable('sensor_data', 'time');

-- é€£ç¶šé›†ç´„
CREATE MATERIALIZED VIEW sensor_data_hourly
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', time) as bucket,
    sensor_id,
    avg(temperature) as avg_temp,
    max(temperature) as max_temp,
    min(temperature) as min_temp
FROM sensor_data
GROUP BY bucket, sensor_id;
```

---

## ğŸ› ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ: æ¥ç¶šã§ããªã„

```bash
# ã‚³ãƒ³ãƒ†ãƒŠã®çŠ¶æ…‹ç¢ºèª
docker-compose ps

# ãƒ­ã‚°ç¢ºèª
docker-compose logs postgres

# ãƒãƒ¼ãƒˆã®ç¢ºèª
netstat -an | findstr 5432
```

### å•é¡Œ: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒé…ã„

1. **ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªã®ç¢ºèª**

```sql
SELECT * FROM pg_stat_statements
ORDER BY total_exec_time DESC
LIMIT 10;
```

2. **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¢ºèª**

```sql
-- æœªä½¿ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan
FROM pg_stat_user_indexes
WHERE idx_scan = 0;
```

3. **VACUUM ã®å®Ÿè¡Œ**

```sql
VACUUM ANALYZE;
```

### å•é¡Œ: ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³

```sql
-- ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
DELETE FROM old_table WHERE created_at < NOW() - INTERVAL '1 year';

-- VACUUM FULL ã§ãƒ‡ã‚£ã‚¹ã‚¯é ˜åŸŸå›å
VACUUM FULL table_name;
```

### å•é¡Œ: è¨­å®šå¤‰æ›´ãŒåæ˜ ã•ã‚Œãªã„

```bash
# è¨­å®šå†èª­ã¿è¾¼ã¿
docker-compose exec postgres psql -U postgres -c "SELECT pg_reload_conf();"

# ã¾ãŸã¯å†èµ·å‹•
docker-compose restart postgres
```

---

## ğŸ“š å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

- [PostgreSQL å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://www.postgresql.org/docs/)
- [PostgreSQL ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰](https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server)
- [PgTune](https://pgtune.leopard.in.ua/) - è¨­å®šè‡ªå‹•ç”Ÿæˆãƒ„ãƒ¼ãƒ«
- [pgAdmin ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://www.pgadmin.org/docs/)
- [explain.depesz.com](https://explain.depesz.com/) - ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³å¯è¦–åŒ–

---

## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å­¦ç¿’ãƒ»é–‹ç™ºç”¨é€”ã«è‡ªç”±ã«ä½¿ç”¨ã§ãã¾ã™ã€‚

---

## ğŸ¤ è²¢çŒ®

æ”¹å–„ææ¡ˆã‚„ãƒã‚°å ±å‘Šã¯ Issue ã¾ãŸã¯ Pull Request ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚
